# -*- coding: utf-8 -*-
"""Assignment_intern.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_XiP2stXEaKdhLHYnVJ2c8qNtrZsOo7P
"""

#All required Liberaries imported here

import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
import os
import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize, sent_tokenize
import re
import chardet
from tqdm import tqdm
import logging


# Load the input file using pandas dataframe
input_file = 'Input.xlsx'
df = pd.read_excel(input_file)

# Display the first two rows to see data dimensions and structure
print(df.head(2))

# Cleaning of all text which received after webscraping

def clean_text(text):
    # Remove contact details or any unnecessary information
    text = re.sub(r'Contact Details.*', '', text, flags=re.DOTALL)
    text = re.sub(r'This solution was designed and developed by.*', '', text, flags=re.DOTALL)

    # Remove URLs
    text = re.sub(r'http\S+|www.\S+', '', text)

    # Remove excessive whitespace, newlines, and tabs
    text = re.sub(r'\s+', ' ', text).strip()

    # Remove any remaining special characters (optional, depending on requirement)
    text = re.sub(r'[^\w\s.,;!?()-]', '', text)

    return text

#Now Extracting the article text using beutifulsoup library by Url given in the dataset

# Configure logging
logging.basicConfig(level=logging.INFO)



# Now extracting the article text using BeautifulSoup library by URL given in the dataset
def extract_article_text(url, retries=3, timeout=10):
    for attempt in range(retries):
        try:
            response = requests.get(url, timeout=timeout)
            response.raise_for_status()  # Raises HTTPError for bad responses
            soup = BeautifulSoup(response.text, 'html.parser')

            # Extract the title
            title = soup.find('h1').get_text() if soup.find('h1') else "No Title Found"

            # Extract the article text
            paragraphs = soup.find_all('div', class_='td-post-content tagdiv-type')
            article_text = ' '.join([p.get_text() for p in paragraphs])

            article_text = clean_text(article_text)
            return title, article_text

        except requests.exceptions.RequestException as e:
            logging.error(f"Attempt {attempt+1} failed for {url}: {e}")
            if attempt + 1 < retries:
                time.sleep(2)
            else:
                logging.error(f"Failed to retrieve {url} after {retries} attempts.")
                return None, None



# Process each row in the dataset
for index, row in tqdm(df.iterrows(), total=df.shape[0]):
    url = row['URL']
    url_id = row['URL_ID']
    title, article_text = extract_article_text(url)
    logging.info(f"Title: {title}\n")

    if title and article_text:  # Ensure both title and text were extracted
        filename = f"{url_id}.txt"
        directory = 'extracted_material'
        # Create the directory if it doesn't exist
        if not os.path.exists(directory):
            os.makedirs(directory)
        filepath = os.path.join(directory, filename)
        with open(filepath, 'w', encoding='utf-8') as file:
            file.write(f"{title}\n\n{article_text}")
        logging.info(f"Article saved to {filename}")
    else:
        logging.warning(f"Skipping URL {url} due to extraction failure.")







#To check which type of Encoding used in given text file
with open('MasterDictionary/negative-words.txt', 'rb') as file:
    raw_data = file.read()
    result = chardet.detect(raw_data)
    encoding = result['encoding']
    print(f"Detected encoding: {encoding}")

with open('MasterDictionary/negative-words.txt', 'r', encoding='ISO-8859-1') as file:
    negative_words = set(file.read().splitlines())
with open('MasterDictionary/positive-words.txt', 'r', encoding='ISO-8859-1') as file:
    positive_words = set(file.read().splitlines())

stop_words = set()
for filename in ['StopWords_Auditor', 'StopWords_Currencies', 'StopWords_DatesandNumbers', 'StopWords_Geographic', 'StopWords_GenericLong','StopWords_Names','StopWords_Generic']:
    with open(f'Stopwords/{filename}.txt', 'r', encoding='ISO-8859-1') as file:
        stop_words.update(file.read().splitlines())



def count_syllables(word):
    vowels = 'aeiouy'
    word = word.lower().strip()  # Convert word to lowercase and remove any surrounding whitespace
    count = 0

    # Handle case where word might be empty after stripping
    if len(word) == 0:
        return count

    # Check if the first character is a vowel
    if word[0] in vowels:
        count += 1

    # Iterate through the word to count the syllables
    for index in range(1, len(word)):
        if word[index] in vowels and word[index - 1] not in vowels:
            count += 1

    # Adjust for words ending with 'e' (silent 'e' rule)
    if word.endswith('e'):
        count -= 1

    # Ensure the syllable count is at least 1
    if count == 0:
        count = 1

    return count



def text_analysis(text):
    # Tokenize the text
    words = word_tokenize(text.lower())
    words_filtered = [word for word in words if word not in stop_words]

    # Calculate positive and negative scores

    positive_score = sum(1 for word in words_filtered if word in positive_words)

    negative_score = sum(1 for word in words_filtered if word in negative_words)

    # Calculate polarity score
    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)

    # Calculate subjectivity score
    subjectivity_score = (positive_score + negative_score) / (len(words_filtered) + 0.000001)

    # Calculate average sentence length

    sentences = sent_tokenize(text)
    avg_sentence_length = len(words_filtered) / len(sentences)

    # Calculate percentage of complex words

    # Call count_syllables and handle potential None return values
    complex_word_count = sum(1 for word in words_filtered if count_syllables(word) and count_syllables(word) > 2)

    percentage_complex_words = complex_word_count / len(words_filtered)

    # Calculate Fog Index
    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)

    # Calculate average number of words per sentence
    avg_words_per_sentence = len(words_filtered) / len(sentences)



    # Handle potential None return values from count_syllables
    syllables_per_word = sum(count_syllables(word) or 0 for word in words_filtered) / len(words_filtered)

    personal_pronouns = sum(1 for word in words if word in ['I', 'me', 'my', 'mine', 'you', 'your', 'yours', 'he', 'him', 'his', 'she', 'her', 'hers', 'it', 'its', 'we', 'us', 'our', 'ours', 'they', 'them', 'their', 'theirs']
)

    avg_word_length = sum(len(word) for word in words_filtered) / len(words_filtered)

    return {
        "POSITIVE SCORE": positive_score,
        "NEGATIVE SCORE": negative_score,
        "POLARITY SCORE": polarity_score,
        "SUBJECTIVITY SCORE": subjectivity_score,
        "AVG SENTENCE LENGTH": avg_sentence_length,
        "PERCENTAGE OF COMPLEX WORDS": percentage_complex_words,
        "FOG INDEX": fog_index,
        "AVG NUMBER OF WORDS PER SENTENCE": avg_words_per_sentence,
        "COMPLEX WORD COUNT": complex_word_count,
        "WORD COUNT": len(words_filtered),
        "SYLLABLE PER WORD": syllables_per_word,
        "PERSONAL PRONOUNS": personal_pronouns,
        "AVG WORD LENGTH": avg_word_length,
    }

urls =df['URL']
results = []
for index, url in enumerate(urls):
    with open(f"extracted_material/{df['URL_ID'][index]}.txt", "r", encoding='ISO-8859-1') as file:
        text = file.read()
    analysis_result = text_analysis(text)
    analysis_result=dict([('URL_ID',df['URL_ID'][index]),('URL',url)]+list(analysis_result.items()))
    results.append(analysis_result)

print(analysis_result) #Last row result

#Saving (results) output file in .xlsx form (Excel sheet) fromat.

output_df = pd.DataFrame(results)
output_df.to_excel('Output Data Structure.xlsx', index=False)
print("Successfully saved in Output Data Structure.xlsx")
